import json
import torch
import re
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import Dict, Any

# ç¡®ä¿è¿™é‡Œçš„è·¯å¾„ä¸æ‚¨ SFT æ¨¡å‹ä¿å­˜çš„ output_dir ä¸€è‡´
MODEL_PATH = "/home/ubuntu/finetune_e2e_new/model" 
TEST_FILE_PATH = "/home/ubuntu/finetune_e2e_new/data/test_intrinsic.json"
OUTPUT_FILE_PATH = "/home/ubuntu/finetune_e2e_new/data/preds_intrinsic.json"

# -----------------------------------------------------------------
# 1. JSON æå–å™¨
# -----------------------------------------------------------------
def extract_json(raw: str) -> Dict[str, Any] | None:
    """Uses regex to find and parse a JSON object in the raw output."""
    try:
        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ª '{' å’Œæœ€åä¸€ä¸ª '}'ï¼Œæå–ä¸­é—´çš„ JSON å­—ç¬¦ä¸²
        match = re.search(r"\{[\s\S]*\}", raw)
        if not match:
            return None
        return json.loads(match.group(0))
    except json.JSONDecodeError:
        return None
    except Exception:
        return None


# -----------------------------------------------------------------
# 2. æ¨ç†ç”Ÿæˆå‡½æ•° (ä¸è®­ç»ƒæ¨¡æ¿ä¸€è‡´)
# -----------------------------------------------------------------
def generate_analysis(buggy_code: str, problem_content: str, model, tokenizer, max_len: int) -> Dict[str, Any]:
    """
    ä½¿ç”¨ SFT åçš„æ¨¡å‹ç”Ÿæˆ bug_span, bug_summary å’Œ planner_text çš„ JSON åˆ†æç»“æœã€‚
    """
    
   # --- 1. SYSTEM PROMPT (3 é˜¶æ®µæŒ‡ä»¤) ---
    system_msg = (
        """You are a strict 3-stage code analysis model.
    You must follow the stages in order and output ONLY the final JSON object.

    STAGE 1: Based on the buggy code -> Generate bug_span (A list of line ranges for the error, format: [start, end] lines).
    STAGE 2: Using the buggy code and bug_span -> Generate bug_summary (A concise summary of the bug in English).
    STAGE 3: Using the buggy code, bug_span, and bug_summary -> Generate planner_text (The step-by-step repair plan in English).

    RULES:
    - You MUST generate all 3 fields: bug_span, bug_summary, and planner_text.
    - The output MUST be, and ONLY be, a single JSON object.
    - All text output (summary and planner_text) MUST be in English.
    """
    )

    user_msg = (
        "### é—®é¢˜æè¿°\n"
        f"{problem}\n\n"
        "### é”™è¯¯ä»£ç \n"
        f"{code}\n\n"  # <-- å°† 'buggy' æ›¿æ¢ä¸º 'code'
        "è¯·éµå¾ª 3 é˜¶æ®µå†…éƒ¨æµç¨‹ï¼Œå¹¶åªè¾“å‡ºæœ€ç»ˆçš„ JSON å¯¹è±¡ã€‚"
    )
    
    messages = [
        {"role": "system", "content": system_msg},
        {"role": "user", "content": user_msg},
    ]

    # åº”ç”¨ Chat æ¨¡æ¿å¹¶ç”Ÿæˆ prompt (add_generation_prompt=True)
    prompt = tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True 
    )

    # æ¨ç†é…ç½®
    input_ids = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=max_len).input_ids.to(model.device)
    
    # è®¾ç½®ç”Ÿæˆå‚æ•°ï¼šå¼ºåˆ¶ä½¿ç”¨è´ªå©ªæœç´¢ï¼Œæ§åˆ¶ JSON ç»“æŸ
    generation_config = model.generation_config
    generation_config.max_new_tokens = 2048
    # 1. å¯ç”¨é‡‡æ ·
    generation_config.do_sample = True 
    # 2. è°ƒæ•´æ¸©åº¦
    generation_config.temperature = 0.8 
    # 3. å¯ç”¨ Top-p é‡‡æ ·
    generation_config.top_p = 0.9 
    # 4. å¯ç”¨ Top-k é‡‡æ ·
    generation_config.top_k = 50  
    
    # ç¡®ä¿åœ¨ } ç»“æŸæˆ–æ ‡å‡† EOS ç»“æŸ
    generation_config.eos_token_id = [
        tokenizer.eos_token_id,              
        tokenizer.convert_tokens_to_ids('}') 
    ] 
    
    # ç”Ÿæˆ
    with torch.no_grad():
        output_ids = model.generate(
            input_ids,
            generation_config=generation_config
        )

    # è§£ç  (åªå–ç”Ÿæˆéƒ¨åˆ†)
    response_text = tokenizer.decode(output_ids[0, input_ids.shape[1]:], skip_special_tokens=True)
    
    # åå¤„ç†
    analysis_result = extract_json(response_text)
    
    if analysis_result is None:
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›åŒ…å«é»˜è®¤å€¼çš„å­—å…¸
        return {"bug_span": [], "bug_summary": "parse_error", "planner_text": "parse_error", "raw_output": response_text}
    
    return analysis_result


# -----------------------------------------------------------------
# 3. ä¸»ç¨‹åºï¼šæ‰¹é‡å¤„ç†
# -----------------------------------------------------------------
if __name__ == "__main__":
    
    # ä½¿ç”¨è®­ç»ƒé…ç½®ä¸­çš„ max_length
    MAX_SEQ_LENGTH = 4096 
    
    # 1. åŠ è½½æ¨¡å‹å’Œ tokenizer
    try:
        print(f"Loading model from {MODEL_PATH}...")
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        model.eval()
    except Exception as e:
        print(f"Error loading model or tokenizer: {e}")
        exit()

    # 2. åŠ è½½æµ‹è¯•æ•°æ®
    try:
        with open(TEST_FILE_PATH, 'r') as f:
            test_data = json.load(f)
        print(f"Loaded {len(test_data)} test samples from {TEST_FILE_PATH}")
    except Exception as e:
        print(f"Error loading test file: {e}")
        exit()

    results = []

    # 3. æ‰¹é‡æ¨ç†
    for idx, example in enumerate(test_data):
        print(f"\n=== Processing sample {idx+1}/{len(test_data)} ===")
        
        problem = example["question_content"]
        code = example["code_list"][0]

        # è¿è¡Œç”Ÿæˆ
        analysis_dict = generate_analysis(code, problem, model, tokenizer, MAX_SEQ_LENGTH)
        
        # ğŸš¨ å…³é”®ä¿®å¤ï¼šæ‰å¹³åŒ–ç»“æœï¼Œåªä¿ç•™ question_title å’Œé¢„æµ‹çš„ key/value
        results.append({
            "question_title": example["question_title"],
            "bug_span": analysis_dict.get("bug_span", []),
            "bug_summary": analysis_dict.get("bug_summary", "parse_error"),
            "planner_text": analysis_dict.get("planner_text", "parse_error"),
        })

        print(f"  > Predicted bug_span: {results[-1]['bug_span']}")
        print(f"  > Predicted bug_summary: {results[-1]['bug_summary']}")
        print(f"  > Predicted planner_text: {results[-1]['planner_text']}")

    # 4. ä¿å­˜ç»“æœ
    with open(OUTPUT_FILE_PATH, "w") as f:
        json.dump(results, f, indent=4, ensure_ascii=False)
    print(f"\n--- Inference Complete. Results saved to {OUTPUT_FILE_PATH} ---")